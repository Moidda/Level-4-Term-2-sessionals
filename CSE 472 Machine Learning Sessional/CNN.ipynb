{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdSoWW9w7IA3"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "GZ5tcn-y7OFS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import sys\n",
        "import os\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "from time import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vg3ypjPoLR8q"
      },
      "source": [
        "# Vectorized Convolution\n",
        "\n",
        "[Github link](https://github.com/slvrfn/vectorized_convolution/blob/master/convolution.py)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZnnXFRiLRpS",
        "outputId": "596e0f43-71ce-4995-b1bd-846769e15664"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "x:  (4, 3, 12, 10)\n",
            "d_out:  (4, 128, 6, 5)\n",
            "conv_out:  (4, 128, 6, 5)\n",
            "db:  (128,)\n",
            "dw:  (128, 3, 3, 3)\n",
            "dx:  (4, 3, 12, 10)\n"
          ]
        }
      ],
      "source": [
        "def getWindows(input, output_size, kernel_size, padding=0, stride=1, dilate=0):\n",
        "    working_input = input\n",
        "    working_pad = padding\n",
        "    # dilate the input if necessary\n",
        "    if dilate != 0:\n",
        "        working_input = np.insert(working_input, range(1, input.shape[2]), 0, axis=2)\n",
        "        working_input = np.insert(working_input, range(1, input.shape[3]), 0, axis=3)\n",
        "\n",
        "    # pad the input if necessary\n",
        "    if working_pad != 0:\n",
        "        working_input = np.pad(working_input, pad_width=((0,), (0,), (working_pad,), (working_pad,)), mode='constant', constant_values=(0.,))\n",
        "\n",
        "    in_b, in_c, out_h, out_w = output_size\n",
        "    out_b, out_c, _, _ = input.shape\n",
        "    batch_str, channel_str, kern_h_str, kern_w_str = working_input.strides\n",
        "\n",
        "    return np.lib.stride_tricks.as_strided(\n",
        "        working_input,\n",
        "        (out_b, out_c, out_h, out_w, kernel_size, kernel_size),\n",
        "        (batch_str, channel_str, stride * kern_h_str, stride * kern_w_str, kern_h_str, kern_w_str)\n",
        "    )\n",
        "\n",
        "\n",
        "class Conv2D:\n",
        "    \"\"\"\n",
        "    An implementation of the convolutional layer. We convolve the input with out_channels different filters\n",
        "    and each filter spans all channels in the input.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0, weight=None, bias=None):\n",
        "        \"\"\"\n",
        "        :param in_channels: the number of channels of the input data\n",
        "        :param out_channels: the number of channels of the output(aka the number of filters applied in the layer)\n",
        "        :param kernel_size: the specified size of the kernel(both height and width)\n",
        "        :param stride: the stride of convolution\n",
        "        :param padding: the size of padding. Pad zeros to the input with padding size.\n",
        "        \"\"\"\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "\n",
        "        self.cache = None\n",
        "\n",
        "        self._init_weights(weight, bias)\n",
        "\n",
        "    def _init_weights(self, weight, bias):\n",
        "        self.weight = 1e-3 * np.random.randn(self.out_channels, self.in_channels,  self.kernel_size, self.kernel_size)\n",
        "        self.bias = np.zeros(self.out_channels)\n",
        "        if weight is not None:\n",
        "            self.weight = weight\n",
        "        if bias is not None:\n",
        "            self.bias = bias\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        The forward pass of convolution\n",
        "        :param x: input data of shape (N, C, H, W)\n",
        "        :return: output data of shape (N, self.out_channels, H', W') where H' and W' are determined by the convolution\n",
        "                 parameters.\n",
        "        \"\"\"\n",
        "\n",
        "        n, c, h, w = x.shape\n",
        "        out_h = (h - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "        out_w = (w - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "\n",
        "        windows = getWindows(x, (n, c, out_h, out_w), self.kernel_size, self.padding, self.stride)\n",
        "\n",
        "        out = np.einsum('bihwkl,oikl->bohw', windows, self.weight)\n",
        "\n",
        "        # add bias to kernels\n",
        "        out += self.bias[None, :, None, None]\n",
        "\n",
        "        self.cache = x, windows\n",
        "        return out\n",
        "\n",
        "    def backward(self, dout):\n",
        "        \"\"\"\n",
        "        The backward pass of convolution\n",
        "        :param dout: upstream gradients\n",
        "        :return: dx, dw, and db relative to this module\n",
        "        \"\"\"\n",
        "        x, windows = self.cache\n",
        "\n",
        "        padding = self.kernel_size - 1 if self.padding == 0 else self.padding\n",
        "\n",
        "        dout_windows = getWindows(dout, x.shape, self.kernel_size, padding=padding, stride=1, dilate=self.stride - 1)\n",
        "        rot_kern = np.rot90(self.weight, 2, axes=(2, 3))\n",
        "\n",
        "        db = np.sum(dout, axis=(0, 2, 3))\n",
        "        dw = np.einsum('bihwkl,bohw->oikl', windows, dout)\n",
        "        dx = np.einsum('bohwkl,oikl->bihw', dout_windows, rot_kern)\n",
        "\n",
        "        return db, dw, dx\n",
        "\n",
        "\n",
        "in_channels = 3\n",
        "out_channels = 128\n",
        "kernel_size = 3\n",
        "stride = 2\n",
        "padding = 1\n",
        "batch_size = (4, in_channels, 12, 10)\n",
        "dout_size = (4, out_channels, 6, 5)\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "x = np.random.random(batch_size)  # create data for forward pass\n",
        "dout = np.random.random(dout_size)  # create random data for backward\n",
        "print('x: ', x.shape)\n",
        "print('d_out: ', dout.shape)\n",
        "\n",
        "conv = Conv2D(in_channels, out_channels, kernel_size, stride, padding)\n",
        "\n",
        "conv_out = conv.forward(x)\n",
        "print('conv_out: ', conv_out.shape)\n",
        "\n",
        "db, dw, dx = conv.backward(dout)\n",
        "print('db: ', db.shape)\n",
        "print('dw: ', dw.shape)\n",
        "print('dx: ', dx.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTA2pVugKOpl"
      },
      "source": [
        "# Convolution Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fuhOvwX6KQp1"
      },
      "outputs": [],
      "source": [
        "class Convolution:\n",
        "    def __init__(self, n_kernels, kernel_dim, stride, padding, learning_rate=0.1, kernels=None, bias=None):\n",
        "        self.n_kernels = n_kernels\n",
        "        self.kernel_dim = kernel_dim\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.kernels = kernels\n",
        "        self.bias = bias\n",
        "        self.input = None\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def zeropad(self, input, padding):\n",
        "        \"\"\"\n",
        "        :param input: array of 2D matrices\n",
        "        :return: array of padded 2D matrices\n",
        "        \"\"\"\n",
        "        n_channels, n_rows, n_cols = input.shape[0], input.shape[1], input.shape[2]\n",
        "        n_rows_out = 2*padding + n_rows\n",
        "        n_cols_out = 2*padding + n_cols\n",
        "        output = np.zeros((n_channels, n_rows_out, n_cols_out))\n",
        "        output[:, padding:padding+n_rows, padding:padding+n_cols] = np.copy(input)\n",
        "        return output\n",
        "\n",
        "    def dilate(self, input, len):\n",
        "        \"\"\"\n",
        "        :param input: array of 2D matrices\n",
        "        :param len: length of dilation\n",
        "        :return: array of dilated 2D matrices\n",
        "        \"\"\"\n",
        "        n_channels, n_rows, n_cols = input.shape\n",
        "        n_rows_out = n_rows + len*(n_rows-1)\n",
        "        n_cols_out = n_cols + len*(n_cols-1) \n",
        "        output = np.zeros((n_channels, n_rows_out, n_cols_out))\n",
        "        output[:,::len+1,::len+1] = input\n",
        "        return output\n",
        "\n",
        "    def rotate180(self, input):\n",
        "        \"\"\"\n",
        "        :param input: an array of 2D matrices (representing one kernel)\n",
        "        :return: an array of 180-degree rotated 2D matrices\n",
        "        \"\"\"\n",
        "        output = np.copy(input)\n",
        "        for i in range(output.shape[0]):\n",
        "            output[i] = np.rot90(output[i])\n",
        "            output[i] = np.rot90(output[i])\n",
        "        return output\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param input: shape(n_channels, n_rows, n_cols)\n",
        "        :returns: shape(n_kernels, n_rows_out, n_cols_out)\n",
        "        self.kernels.shape(n_kernels, n_channels, kernel_dim, kernel_dim)\n",
        "        \"\"\"\n",
        "        self.input = self.zeropad(input, self.padding)\n",
        "        n_channels, n_rows, n_cols = self.input.shape\n",
        "        n_rows_out = (n_rows-self.kernel_dim) // self.stride + 1\n",
        "        n_cols_out = (n_cols-self.kernel_dim) // self.stride + 1\n",
        "        output = np.zeros((self.n_kernels, n_rows_out, n_cols_out))\n",
        "\n",
        "        if self.kernels is None:\n",
        "            self.kernels = np.random.randn(self.n_kernels, self.input.shape[0], self.kernel_dim, self.kernel_dim)\n",
        "            # Xavier initialization\n",
        "            for i in range(self.n_kernels):\n",
        "                self.kernels[i,:,:,:] = np.random.normal(loc=0, scale=np.sqrt(1./(n_channels*self.kernel_dim*self.kernel_dim)), size=(n_channels, self.kernel_dim, self.kernel_dim))\n",
        "\n",
        "        if self.bias is None:\n",
        "            self.bias = np.zeros((self.n_kernels, 1))\n",
        "        \n",
        "        for k in range(self.n_kernels):\n",
        "            for row, row_out in zip(range(0, n_rows, self.stride), range(n_rows_out)):\n",
        "                for col, col_out in zip(range(0, n_cols, self.stride), range(n_cols_out)):\n",
        "                    output[k,row_out,col_out] = np.sum(self.input[:,row:row+self.kernel_dim, col:col+self.kernel_dim] * self.kernels[k,:,:,:]) + self.bias[k]\n",
        "        \n",
        "        return output\n",
        "\n",
        "    def backward(self, dz):\n",
        "        \"\"\"\n",
        "        :param dz: output gradient, from next layer\n",
        "        dx = input gradient, to be passed to previous layer\n",
        "        dk = weights/kernel gradient, for learning the weights\n",
        "        db = bias gradient, for learning the biases\n",
        "        \"\"\"\n",
        "        n_channels, n_rows, n_cols = self.input.shape\n",
        "        n_rows_out = (n_rows-self.kernel_dim) // self.stride + 1\n",
        "        n_cols_out = (n_cols-self.kernel_dim) // self.stride + 1\n",
        "\n",
        "        # Gradient with respect to input\n",
        "        dx = np.zeros((n_channels, n_rows, n_cols))\n",
        "        dz_sparsed = self.dilate(dz, self.stride-1)     \n",
        "        dz_sparsed = self.zeropad(dz_sparsed, self.kernel_dim-1)   \n",
        "        # dx = sum( conv(dz_sparsed,rotated_K) )\n",
        "        for k in range(self.n_kernels):\n",
        "            for row in range(n_rows):\n",
        "                for col in range(n_cols):\n",
        "                    kernel_rotated = self.rotate180(self.kernels[k])\n",
        "                    cell_wise_mult = dz_sparsed[k, row:row+self.kernel_dim, col:col+self.kernel_dim] * kernel_rotated[:,:,:]\n",
        "                    dx[:,row,col] += np.sum(cell_wise_mult, axis=(1,2))\n",
        "\n",
        "        # calculate gradient_kernels\n",
        "        dk = np.zeros(self.kernels.shape)\n",
        "        dz_dilated = self.dilate(dz, self.stride-1)\n",
        "        # dk = conv(input, dilated output gradient)\n",
        "        for k in range(self.n_kernels):\n",
        "            for row in range(self.kernel_dim):\n",
        "                for col in range(self.kernel_dim):\n",
        "                    bostu = self.input[:, row:row+dz_dilated.shape[1], col:col+dz_dilated.shape[2]] * dz_dilated[k]\n",
        "                    bostu = np.sum(bostu, axis=(1,2))\n",
        "                    dk[k,:,row,col] = bostu\n",
        "\n",
        "        # calculate gradient_bias\n",
        "        db = np.zeros((self.n_kernels, 1))\n",
        "        for k in range(self.n_kernels):\n",
        "            db[k,:] = np.sum(dz[k,:,:])\n",
        "\n",
        "        # update parameters\n",
        "        self.kernels -= self.learning_rate*dk\n",
        "        for k in range(self.n_kernels):\n",
        "            self.bias[k] -= self.learning_rate*db[k]\n",
        "        return dx\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CvtBkMqLCpLQ"
      },
      "source": [
        "# Convolution: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 222,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5xHkRIK_CtdY",
        "outputId": "80be0a78-41ca-40e8-f4fa-12ea53292fec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1\n",
            "[[[[15219. 15921.]\n",
            "   [18729. 19431.]]\n",
            "\n",
            "  [[37818. 39978.]\n",
            "   [48618. 50778.]]]]\n",
            "------\n",
            "f2\n",
            "[[[15219. 15921.]\n",
            "  [18729. 19431.]]\n",
            "\n",
            " [[37818. 39978.]\n",
            "  [48618. 50778.]]]\n",
            "------------\n",
            "dk1\n",
            "[[[[  452304.   521604.   590904.]\n",
            "   [  798804.   868104.   937404.]\n",
            "   [ 1145304.  1214604.  1283904.]]\n",
            "\n",
            "  [[ 2184804.  2254104.  2323404.]\n",
            "   [ 2531304.  2600604.  2669904.]\n",
            "   [ 2877804.  2947104.  3016404.]]\n",
            "\n",
            "  [[ 3917304.  3986604.  4055904.]\n",
            "   [ 4263804.  4333104.  4402404.]\n",
            "   [ 4610304.  4679604.  4748904.]]]\n",
            "\n",
            "\n",
            " [[[ 1175472.  1352664.  1529856.]\n",
            "   [ 2061432.  2238624.  2415816.]\n",
            "   [ 2947392.  3124584.  3301776.]]\n",
            "\n",
            "  [[ 5605272.  5782464.  5959656.]\n",
            "   [ 6491232.  6668424.  6845616.]\n",
            "   [ 7377192.  7554384.  7731576.]]\n",
            "\n",
            "  [[10035072. 10212264. 10389456.]\n",
            "   [10921032. 11098224. 11275416.]\n",
            "   [11806992. 11984184. 12161376.]]]]\n",
            "-------\n",
            "dk2\n",
            "------------\n",
            "dx1\n",
            "[[[[ 1021086.  1074123.  2206566.  1135305.  1191204.]\n",
            "   [ 1180197.  1233234.  2533374.  1303002.  1358901.]\n",
            "   [ 2651994.  2772378.  5678568.  2911914.  3038022.]\n",
            "   [ 1514727.  1582074.  3231054.  1651842.  1722051.]\n",
            "   [ 1716768.  1784115.  3643722.  1862469.  1932678.]]\n",
            "\n",
            "  [[ 1498419.  1551456.  3186990.  1638396.  1694295.]\n",
            "   [ 1657530.  1710567.  3513798.  1806093.  1861992.]\n",
            "   [ 3735450.  3855834.  7896996.  4046886.  4172994.]\n",
            "   [ 2120850.  2188197.  4469058.  2283723.  2353932.]\n",
            "   [ 2322891.  2390238.  4881726.  2494350.  2564559.]]\n",
            "\n",
            "  [[ 1975752.  2028789.  4167414.  2141487.  2197386.]\n",
            "   [ 2134863.  2187900.  4494222.  2309184.  2365083.]\n",
            "   [ 4818906.  4939290. 10115424.  5181858.  5307966.]\n",
            "   [ 2726973.  2794320.  5707062.  2915604.  2985813.]\n",
            "   [ 2929014.  2996361.  6119730.  3126231.  3196440.]]]]\n",
            "-------\n",
            "dx2\n",
            "[[[ 1021086.  1074123.  2206566.  1135305.  1191204.]\n",
            "  [ 1180197.  1233234.  2533374.  1303002.  1358901.]\n",
            "  [ 2651994.  2772378.  5678568.  2911914.  3038022.]\n",
            "  [ 1514727.  1582074.  3231054.  1651842.  1722051.]\n",
            "  [ 1716768.  1784115.  3643722.  1862469.  1932678.]]\n",
            "\n",
            " [[ 1498419.  1551456.  3186990.  1638396.  1694295.]\n",
            "  [ 1657530.  1710567.  3513798.  1806093.  1861992.]\n",
            "  [ 3735450.  3855834.  7896996.  4046886.  4172994.]\n",
            "  [ 2120850.  2188197.  4469058.  2283723.  2353932.]\n",
            "  [ 2322891.  2390238.  4881726.  2494350.  2564559.]]\n",
            "\n",
            " [[ 1975752.  2028789.  4167414.  2141487.  2197386.]\n",
            "  [ 2134863.  2187900.  4494222.  2309184.  2365083.]\n",
            "  [ 4818906.  4939290. 10115424.  5181858.  5307966.]\n",
            "  [ 2726973.  2794320.  5707062.  2915604.  2985813.]\n",
            "  [ 2929014.  2996361.  6119730.  3126231.  3196440.]]]\n"
          ]
        }
      ],
      "source": [
        "input = np.reshape(np.arange(75), (3,5,5))\n",
        "kernels = np.reshape(np.arange(54), (2,3,3,3))\n",
        "bias = np.array([0]*2)\n",
        "\n",
        "kernels = kernels.astype('float64')\n",
        "bias = bias.astype('float64')\n",
        "\n",
        "conv2d = Conv2D(input.shape[0], kernels.shape[0], kernels.shape[2], 2, 0, kernels, bias)\n",
        "input_ = np.zeros((1,3,5,5))\n",
        "input_[0] = np.copy(input)\n",
        "f1 = conv2d.forward(input_)\n",
        "db, dk, dx = conv2d.backward(f1) \n",
        "\n",
        "conv = Convolution(kernels.shape[0], kernels.shape[2], 2, 0, 0.1, kernels, bias)\n",
        "f2 = conv.forward(input)\n",
        "dx2= conv.backward(f2)\n",
        "\n",
        "print('f1')\n",
        "print(f1)\n",
        "print('------')\n",
        "print('f2')\n",
        "print(f2)\n",
        "\n",
        "print('------------')\n",
        "print('dk1')\n",
        "print(dk)\n",
        "print('-------')\n",
        "print('dk2')\n",
        "\n",
        "print('------------')\n",
        "print('dx1')\n",
        "print(dx)\n",
        "print('-------')\n",
        "print('dx2')\n",
        "print(dx2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wJkpOuPOz_A"
      },
      "source": [
        "# Max Pooling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "vbg4meT0PB29"
      },
      "outputs": [],
      "source": [
        "class MaxPooling:\n",
        "    def __init__(self, pool_dim, stride):\n",
        "        self.pool_dim = pool_dim\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param input: array of 2D matrices\n",
        "        :return: array of 2D matrices, each matrix maxpool filter applied\n",
        "        \"\"\"\n",
        "        self.input = input\n",
        "        n_channels, n_rows, n_cols = self.input.shape\n",
        "        n_rows_out = (n_rows-self.pool_dim) // self.stride + 1\n",
        "        n_cols_out = (n_cols-self.pool_dim) // self.stride + 1\n",
        "        output = np.zeros((n_channels, n_rows_out, n_cols_out))\n",
        "        for c in range(n_channels):\n",
        "            for row, row_out in zip(range(0, n_rows, self.stride), range(n_rows_out)):\n",
        "                for col, col_out in zip(range(0, n_cols, self.stride), range(n_cols_out)):\n",
        "                    output[c,row_out,col_out] = np.max(self.input[c, row:row+self.pool_dim, col:col+self.pool_dim])\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, dz):\n",
        "        \"\"\"\n",
        "        :param dz: \n",
        "        \"\"\"\n",
        "        n_channels, n_rows, n_cols = dz.shape\n",
        "        dx = np.zeros_like(self.input)\n",
        "        for i in range(n_channels):\n",
        "            for j in range(n_rows):\n",
        "                for k in range(n_cols):\n",
        "                    patch = self.input[i, j*self.stride:j*self.stride+self.pool_dim, k*self.stride:k*self.stride+self.pool_dim]\n",
        "                    max_index = np.unravel_index(np.argmax(patch), patch.shape)\n",
        "                    dx[i, j*self.stride:j*self.stride+self.pool_dim, k*self.stride:k*self.stride+self.pool_dim][max_index] = dz[i, j, k]\n",
        "        return dx\n",
        "        # n_channels, n_rows, n_cols = self.input.shape\n",
        "        # n_rows_out = (n_rows-self.pool_dim) // self.stride + 1\n",
        "        # n_cols_out = (n_cols-self.pool_dim) // self.stride + 1\n",
        "        # dx = np.zeros(self.input.shape)\n",
        "        # for c in range(n_channels):\n",
        "        #     for row, row_out in zip(range(0, n_rows, self.stride), range(n_rows_out)):\n",
        "        #         for col, col_out in zip(range(0, n_cols, self.stride), range(n_cols_out)):\n",
        "        #             st = np.argmax(self.input[c, row:row+self.pool_dim, col:col+self.pool_dim])\n",
        "        #             idx, idy = np.unravel_index(st, (self.pool_dim, self.pool_dim))\n",
        "        #             dx[c, row+idx, col+idy] = dz[c, row_out, col_out]\n",
        "        # return dx\n",
        "        # assert not isinstance(dz, str)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elrIfYt9TI9g"
      },
      "source": [
        "# Max Pool: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 208,
      "metadata": {
        "id": "kQldD_5rTPl8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input ->\n",
            "[[[ 0.  1.  2.  3.  4.]\n",
            "  [ 5.  6.  7.  8.  9.]\n",
            "  [10. 11. 12. 13. 14.]\n",
            "  [15. 16. 17. 18. 19.]\n",
            "  [20. 21. 22. 23. 24.]]\n",
            "\n",
            " [[25. 26. 27. 28. 29.]\n",
            "  [30. 31. 32. 33. 34.]\n",
            "  [35. 36. 37. 38. 39.]\n",
            "  [40. 41. 42. 43. 44.]\n",
            "  [45. 46. 47. 48. 49.]]\n",
            "\n",
            " [[50. 51. 52. 53. 54.]\n",
            "  [55. 56. 57. 58. 59.]\n",
            "  [60. 61. 62. 63. 64.]\n",
            "  [65. 66. 67. 68. 69.]\n",
            "  [70. 71. 72. 73. 74.]]]\n",
            "output ->\n",
            "[[[12. 14.]\n",
            "  [22. 24.]]\n",
            "\n",
            " [[37. 39.]\n",
            "  [47. 49.]]\n",
            "\n",
            " [[62. 64.]\n",
            "  [72. 74.]]]\n",
            "dx ->\n",
            "[[[ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0. 12.  0. 14.]\n",
            "  [ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0. 22.  0. 24.]]\n",
            "\n",
            " [[ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0. 37.  0. 39.]\n",
            "  [ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0. 47.  0. 49.]]\n",
            "\n",
            " [[ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0. 62.  0. 64.]\n",
            "  [ 0.  0.  0.  0.  0.]\n",
            "  [ 0.  0. 72.  0. 74.]]]\n"
          ]
        }
      ],
      "source": [
        "input = np.reshape(np.arange(75), (3,5,5)).astype('float64')\n",
        "maxpooling = MaxPooling(3, 2)\n",
        "output = maxpooling.forward(input)\n",
        "print('input ->\\n' + str(input))\n",
        "print('output ->\\n' + str(output))\n",
        "dx = maxpooling.backward(output)\n",
        "print('dx ->\\n' + str(dx))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "e9DclRKEHm-Y"
      },
      "source": [
        "# ReLU Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "T-QF-IKZHs6K"
      },
      "outputs": [],
      "source": [
        "class Relu:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param input: input to ReLU activation\n",
        "        :returns: max(0,input)\n",
        "        \"\"\"\n",
        "        self.input = input\n",
        "        ret = np.copy(input)\n",
        "        ret[ret<0] = 0\n",
        "        return ret\n",
        "\n",
        "    def backward(self, dz):\n",
        "        dx = np.copy(dz)\n",
        "        dx[self.input<0] = 0\n",
        "        return dx"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "9DEKnSTXI44J"
      },
      "source": [
        "# Softmax\n",
        "\n",
        "When working with exponents, there's a danger of overflow errors if the base gets too large. This can easily happen when working with the output of a linear layer. To protect ourselves from this, we can use a trick described by Paul Panzer on StackOverflow. Because softmax(x) = softmax(x - c) for any constant c, we can calculate $\\sigma(x)_i = \\frac{exp(x_i-max(x))}{\\sum_{j=1}^{N} exp(x_j - max(x))}$ instead"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "akrpXfgjI75y"
      },
      "outputs": [],
      "source": [
        "class Softmax:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):   \n",
        "        exp = np.exp(input-np.max(input), dtype=np.float64)\n",
        "        self.output = exp/np.sum(exp)\n",
        "        return self.output\n",
        "    \n",
        "    def backward(self, dz):\n",
        "        return dz \n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flattening layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Flatten:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :param input: array of 2D matrices\n",
        "        :returns: row vector of shape (1,)\n",
        "        \"\"\"\n",
        "        self.input_shape = input.shape\n",
        "        return np.reshape(input, (1, input.shape[0]*input.shape[1]*input.shape[2]))\n",
        "\n",
        "    def backward(self, dz):\n",
        "        \"\"\"\n",
        "        :returs: array of 2D matrices\n",
        "        \"\"\"\n",
        "        return np.reshape(dz, self.input_shape)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flattening Layer: Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input ->\n",
            "[[[ 0.  1.  2.  3.  4.]\n",
            "  [ 5.  6.  7.  8.  9.]\n",
            "  [10. 11. 12. 13. 14.]\n",
            "  [15. 16. 17. 18. 19.]\n",
            "  [20. 21. 22. 23. 24.]]\n",
            "\n",
            " [[25. 26. 27. 28. 29.]\n",
            "  [30. 31. 32. 33. 34.]\n",
            "  [35. 36. 37. 38. 39.]\n",
            "  [40. 41. 42. 43. 44.]\n",
            "  [45. 46. 47. 48. 49.]]\n",
            "\n",
            " [[50. 51. 52. 53. 54.]\n",
            "  [55. 56. 57. 58. 59.]\n",
            "  [60. 61. 62. 63. 64.]\n",
            "  [65. 66. 67. 68. 69.]\n",
            "  [70. 71. 72. 73. 74.]]]\n",
            "forward ->\n",
            "[[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17.\n",
            "  18. 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35.\n",
            "  36. 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53.\n",
            "  54. 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71.\n",
            "  72. 73. 74.]]\n",
            "backward ->\n",
            "[[[ 0.  1.  2.  3.  4.]\n",
            "  [ 5.  6.  7.  8.  9.]\n",
            "  [10. 11. 12. 13. 14.]\n",
            "  [15. 16. 17. 18. 19.]\n",
            "  [20. 21. 22. 23. 24.]]\n",
            "\n",
            " [[25. 26. 27. 28. 29.]\n",
            "  [30. 31. 32. 33. 34.]\n",
            "  [35. 36. 37. 38. 39.]\n",
            "  [40. 41. 42. 43. 44.]\n",
            "  [45. 46. 47. 48. 49.]]\n",
            "\n",
            " [[50. 51. 52. 53. 54.]\n",
            "  [55. 56. 57. 58. 59.]\n",
            "  [60. 61. 62. 63. 64.]\n",
            "  [65. 66. 67. 68. 69.]\n",
            "  [70. 71. 72. 73. 74.]]]\n"
          ]
        }
      ],
      "source": [
        "input = np.reshape(np.arange(75), (3,5,5)).astype('float64')\n",
        "flatten = Flatten()\n",
        "output = flatten.forward(input)\n",
        "print('input ->\\n' + str(input))\n",
        "print('forward ->\\n' + str(output))\n",
        "print('backward ->\\n' + str(flatten.backward(output)))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fully Connected Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FullyConnected:\n",
        "    def __init__(self, n_output, learning_rate=0.01):\n",
        "        \"\"\"\n",
        "        :n_output: no of outputs\n",
        "        :input: shape(1,n_input); dynamically assigned during forward\n",
        "        :weights: shape(n_inputs, n_outputs)\n",
        "        :bias: shape(1,n_outputs)\n",
        "        \"\"\"\n",
        "        self.n_output = n_output\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        self.input = None\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def setParams(self, weights, bias):\n",
        "        self.weights = weights\n",
        "        self.bias = bias\n",
        "\n",
        "    def forward(self, input):\n",
        "        \"\"\"\n",
        "        :input: shape(1, n_input)\n",
        "        :weight: shape(n_input, n_output)\n",
        "        :bias: shape(1, n_output)\n",
        "        :return: shape(1, n_output)\n",
        "        \"\"\"\n",
        "        self.input = input\n",
        "        n_input = input.shape[1]\n",
        "\n",
        "        if self.weights is None:\n",
        "            # Xavier initialization\n",
        "            self.weights = np.random.normal(loc=0, scale=np.sqrt(1./(n_input*self.n_output)), size=(n_input, self.n_output))\n",
        "\n",
        "        if self.bias is None:\n",
        "            self.bias = np.zeros((1, self.n_output))\n",
        "\n",
        "        return np.dot(self.input, self.weights) + self.bias\n",
        "\n",
        "    def backward(self, dz):\n",
        "        \"\"\"\n",
        "        :dz: shape(1, n_output)\n",
        "        :dw: shape(n_input, n_output)\n",
        "        :dx: shape(1, n_input)\n",
        "        \"\"\"\n",
        "        dw = np.dot(self.input.T, dz)           # (n_input,n_output) = (n_input,1) DOT (1,n_output)\n",
        "        dx = np.dot(dz, self.weights.T)         # (1,n_input) = (1,n_output) DOT (n_output,n_input)\n",
        "        db = np.sum(dz, axis=1, keepdims=True)  # (1,n_output) = (1,n_output)\n",
        "\n",
        "        self.weights -= self.learning_rate * dw\n",
        "        self.bias -= self.learning_rate * db\n",
        "\n",
        "        return dx\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Fully Connected Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input ->\n",
            "[[0. 1. 2. 3.]]\n",
            "weights ->\n",
            " [[0.   1.   2.  ]\n",
            " [2.58 3.52 4.46]\n",
            " [5.16 6.04 6.92]\n",
            " [7.74 8.56 9.38]]\n",
            "bias -> \n",
            "[ -6930.  -17719.2]\n",
            "forward ->\n",
            "[[42. 48. 54.]]\n",
            "backward -> \n",
            "[[ 156.  588. 1020. 1452.]]\n"
          ]
        }
      ],
      "source": [
        "input = np.reshape(np.arange(4), (1,4)).astype('float64')\n",
        "\n",
        "weights = np.reshape(np.arange(12), (4,3)).astype('float64')\n",
        "# bias = np.reshape(np.arange(3), (1,3)).astype('float64')\n",
        "\n",
        "\n",
        "fc = FullyConnected(3)\n",
        "fc.setParams(weights, None)\n",
        "output = fc.forward(input)\n",
        "\n",
        "dx = fc.backward(output)\n",
        "\n",
        "print('input ->\\n' + str(input))\n",
        "print('weights ->\\n ' + str(weights))\n",
        "print('bias -> \\n' + str(bias))\n",
        "print('forward ->\\n' + str(output))\n",
        "print('backward -> \\n' + str(dx))"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cross Entropty Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossEntropyLoss(y_pred, y_true):\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy loss between the true labels and predicted probabilities.\n",
        "\n",
        "    Parameters:\n",
        "        y_true (np.array): A row matrix of shape (1, n_classes) representing the true labels.\n",
        "        y_pred (np.array): A row matrix of shape (1, n_classes) representing the predicted probabilities.\n",
        "\n",
        "    Returns:\n",
        "        float: The cross-entropy loss.\n",
        "    \"\"\"\n",
        "    true_class = np.argmax(y_true, axis=1)\n",
        "    loss = -np.log(y_pred[0, true_class])\n",
        "    return np.sum(loss)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadImage(image_name):\n",
        "    \"\"\"\n",
        "    :param image_name: filename of the image\n",
        "    :returns: array of 2D matrices (1 channels) shape(1,h,w)\n",
        "    Returns the grayscaled pixel values\n",
        "    \"\"\"\n",
        "    img = Image.open(image_name).convert('L')\n",
        "    img = img.resize((32,32))\n",
        "    data = np.asarray(img)\n",
        "    data = np.reshape(data, (1,32,32))\n",
        "    return data\n",
        "\n",
        "def oneHotEncode(digit):\n",
        "    output = np.zeros((1, 10))\n",
        "    output[0,digit] = 1.0\n",
        "    return output\n",
        "\n",
        "def oneHotDecode(onehot):\n",
        "    return np.argmax(onehot)\n",
        "\n",
        "def readcsv(csv_file_name):\n",
        "    df = pd.read_csv(csv_file_name)\n",
        "    X = []\n",
        "    Y = []\n",
        "    for index, row in df.iterrows():\n",
        "        image_file_name = './Dataset/' + row['database name'] + '/' + row['filename']\n",
        "        digit = row['digit']\n",
        "        data = loadImage(image_file_name)\n",
        "        label = oneHotEncode(digit)\n",
        "        X.append(data)\n",
        "        Y.append(label)\n",
        "    return np.asarray(X), np.asarray(Y)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Data Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 189,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 189,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "oneHotDecode(np.reshape(np.array([0,.8,0,.9,0,0,0,0,0,0]), (1,10)))\n",
        "\n",
        "X, Y = readcsv('./Dataset/training-d.csv')\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)\n",
        "\n",
        "for x in X:\n",
        "    assert x.shape == X[0].shape, str(x.shape) + '!=' + str(X[0].shape)\n",
        "\n",
        "# print(X)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "class Network:\n",
        "    def __init__(self):\n",
        "        self.layers = []\n",
        "    \n",
        "    def addLayer(self, layer):\n",
        "        self.layers.append(layer)\n",
        "\n",
        "    def batchNormalize(self, X):\n",
        "        return (X-np.mean(X)) / np.std(X)\n",
        "\n",
        "    def train(self, X_train, Y_train, batch_size, epoch):\n",
        "        \"\"\"\n",
        "        :param X_train: training data. \n",
        "            Each input is a grayscaled image of 32x32 pixels \n",
        "            shape(n_inputs, 1,32,32)\n",
        "        :param Y_train: label of each training data\n",
        "            Each label is one-hot encoded\n",
        "            shape(n_inputs, 1,10)\n",
        "        :patam batch_size: the number of inputs to be taken in a single batch\n",
        "        :param epoch: epoch\n",
        "        :returns:\n",
        "        \"\"\"\n",
        "        # Number of input samples\n",
        "        n_inputs = X_train.shape[0]\n",
        "\n",
        "        for e in tqdm(range(epoch), desc='Epoch', position=0):\n",
        "            loss_training = 0\n",
        "            \n",
        "            # Shuffle the training data\n",
        "            perm = np.random.permutation(n_inputs)\n",
        "            X_train = X_train[perm]\n",
        "            Y_train = Y_train[perm]\n",
        "            \n",
        "            # Split training data into mini-batches\n",
        "            for i in tqdm(range(0, n_inputs, batch_size), desc='Batch', position=1, leave=False):\n",
        "                X_batch = X_train[i : np.minimum(i+batch_size, n_inputs)]\n",
        "                Y_batch = Y_train[i : np.minimum(i+batch_size, n_inputs)]\n",
        "                \n",
        "                start_time_batch = time()\n",
        "                # Take a single input X and respective label Y (one-hot-encoded)\n",
        "                for X, Y in zip(X_batch, Y_batch):\n",
        "                    # Forward through the layers\n",
        "                    output = self.batchNormalize(X)\n",
        "                    for layer in self.layers:\n",
        "                        output = layer.forward(output)\n",
        "            \n",
        "                    loss_training += crossEntropyLoss(output, Y)\n",
        "\n",
        "                    # Backward through the layers\n",
        "                    output_gradient = (output - np.copy(Y)) / batch_size\n",
        "                    for layer in reversed(self.layers):\n",
        "                        output_gradient = layer.backward(output_gradient)\n",
        "                \n",
        "                end_time_batch = time()\n",
        "                time_batch = end_time_batch-start_time_batch\n",
        "                remaining_time = (n_inputs*epoch - n_inputs*e - i)/batch_size * time_batch\n",
        "                hours, remainder = divmod(remaining_time, 3600)\n",
        "                minutes, seconds = divmod(remainder, 60)\n",
        "\n",
        "            loss_training /= n_inputs\n",
        "\n",
        "            # Perform validation on the training dataset to get different metrics\n",
        "            loss_validation, accuracy, f1, c_matrix = self.validate(X_train, Y_train)\n",
        "\n",
        "            print(\"Epoch:{0:2d}/{1:2d} | Loss Training:{2:.5f} | Loss Validation:{3:.5f} | Accuracy:{4:.5f} | F1:{5:5f} | ETA:{6:2d}:{7:2d}:{8:2d}\"\n",
        "            .format(e, epoch, loss_training, loss_validation, accuracy, f1, int(hours), int(minutes), int(seconds)))\n",
        "            print('---Confusion Matrix---\\n' + str(c_matrix))\n",
        "\n",
        "\n",
        "    def validate(self, X_test, Y_test):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            X_test:\n",
        "            Y_test: array of row matrices\n",
        "        \"\"\"\n",
        "        loss = 0\n",
        "        Y_pred_list = []\n",
        "        for X, Y in zip(X_test, Y_test):\n",
        "            output = self.batchNormalize(X)\n",
        "            # forward pass\n",
        "            for layer in self.layers:\n",
        "                output = layer.forward(output)\n",
        "            loss += crossEntropyLoss(output, Y)            \n",
        "            Y_pred_list.append(oneHotDecode(output))\n",
        "\n",
        "        Y_test_list = list(map(oneHotDecode, Y_test[:,0,:]))\n",
        "        assert len(Y_test_list) == len(Y_pred_list), str(len(Y_test_list)) + '!=' + str(len(Y_pred_list))\n",
        "        \n",
        "        loss /= X_test.shape[0]\n",
        "        accuracy = accuracy_score(Y_test_list, Y_pred_list)\n",
        "        f1 = f1_score(Y_test_list, Y_pred_list, average='macro')\n",
        "        c_matrix = confusion_matrix(Y_test_list, Y_pred_list)\n",
        "\n",
        "        return loss, accuracy, f1, c_matrix\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Parameters:\n",
        "            X: a single input\n",
        "\n",
        "        Returns:\n",
        "            d: a digit representing the prediction\n",
        "        \"\"\"\n",
        "        output = self.batchNormalize(X)\n",
        "        for layer in self.layers:\n",
        "            output = layer.forward(output)\n",
        "        return oneHotDecode(output)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Network Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch:   0%|          | 0/5 [18:43<?, ?it/s]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16192\\646051810.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreadcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Dataset/training-b.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m \u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16192\\1853668011.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, Y_train, batch_size, epoch)\u001b[0m\n\u001b[0;32m     50\u001b[0m                     \u001b[0moutput_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                     \u001b[1;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                         \u001b[0moutput_gradient\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput_gradient\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mend_time_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16192\\3251752777.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, dz)\u001b[0m\n\u001b[0;32m     93\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_cols\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m                     \u001b[0mkernel_rotated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrotate180\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m                     \u001b[0mcell_wise_mult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdz_sparsed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_dim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_dim\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkernel_rotated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m                     \u001b[0mdx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcell_wise_mult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16192\\3251752777.py\u001b[0m in \u001b[0;36mrotate180\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     42\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m             \u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrot90\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "network = Network()\n",
        "network.addLayer(Convolution(n_kernels=6, kernel_dim=5, stride=1, padding=0))\n",
        "network.addLayer(Relu())\n",
        "network.addLayer(MaxPooling(pool_dim=2, stride=2))\n",
        "\n",
        "network.addLayer(Convolution(n_kernels=16, kernel_dim=5, stride=1, padding=0))\n",
        "network.addLayer(Relu())\n",
        "network.addLayer(MaxPooling(pool_dim=2, stride=2))\n",
        "\n",
        "network.addLayer(Convolution(n_kernels=120, kernel_dim=5, stride=1, padding=0, learning_rate=0.0001))\n",
        "network.addLayer(Relu())\n",
        "\n",
        "network.addLayer(Flatten())\n",
        "\n",
        "network.addLayer(FullyConnected(n_output=84))\n",
        "network.addLayer(Relu())\n",
        "\n",
        "network.addLayer(FullyConnected(n_output=10))\n",
        "network.addLayer(Relu())\n",
        "\n",
        "network.addLayer(Softmax())\n",
        "\n",
        "X_train, Y_train = readcsv('./Dataset/training-b.csv')\n",
        "network.train(X_train, Y_train, 128, 5)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Store the model in pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle_file = open('1705085_model.pickle', 'wb')\n",
        "pickle.dump(network, pickle_file)\n",
        "pickle_file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load the model from pickle file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "pickle_file = open('1705085_model.pickle', 'rb')\n",
        "network = pickle.load(pickle_file)\n",
        "pickle_file.close()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Prediction of all images in a folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_file_content = 'FileName,Digit\\n'\n",
        "\n",
        "folder_name = './Dataset/training-b/'\n",
        "for entry in os.scandir(folder_name):\n",
        "    if entry.is_file():\n",
        "        image_path = folder_name + entry.name\n",
        "        X = loadImage(image_path)\n",
        "        prediction = network.predict(X)\n",
        "        pred_file_content += entry.name + ',' + str(prediction) + '\\n'\n",
        "\n",
        "pred_file_name = '170585_prediction.csv'\n",
        "pred_file = open(pred_file_name, 'w')\n",
        "pred_file.write(pred_file_content)\n",
        "pred_file.close()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vg3ypjPoLR8q",
        "CvtBkMqLCpLQ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "576cc233f74d4f79b2b51360e557e116b46578149335f87f204b720cd69e6010"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
